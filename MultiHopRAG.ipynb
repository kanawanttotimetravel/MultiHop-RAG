{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanawanttotimetravel/MultiHop-RAG/blob/main/MultiHopRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/sample_data"
      ],
      "metadata": {
        "id": "dB-DFuJJRyhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Read and split the document into passages\n",
        "def load_corpus(path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "    return [p.strip() for p in text.split('<endofpassage>') if p.strip()]\n",
        "\n",
        "passages = load_corpus(\"/content/multihoprag_corpus.txt\")\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "corpus_embeddings = embedder.encode(passages, convert_to_tensor=True)\n",
        "\n",
        "import torch\n",
        "import heapq\n",
        "\n",
        "def retrieve_topk(query, k=2):\n",
        "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
        "    cosine_scores = torch.nn.functional.cosine_similarity(query_embedding, corpus_embeddings)\n",
        "    top_k_indices = torch.topk(cosine_scores, k).indices\n",
        "    return [passages[i] for i in top_k_indices]\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen3-1.7B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             trust_remote_code=True,\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             device_map=\"auto\").eval()\n"
      ],
      "metadata": {
        "id": "ldJJFWNaitWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(query):\n",
        "    context = retrieve_topk(query)\n",
        "    system = \"Please answer the question based on the contexts. Only generate the question and nothing else.\"\n",
        "    prompt = f\"System:\\n{system}\\nContext:\\n{context[0]}\\n{context[1]}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    output = model.generate(**inputs, max_new_tokens=100)\n",
        "    full_response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    # Extract only the answer part after \"Answer:\"\n",
        "    answer = full_response.split(\"Answer:\")[-1].strip().split('\\n')[0]\n",
        "    return answer"
      ],
      "metadata": {
        "id": "aqMw3Kfll2pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    # Simple whitespace tokenization\n",
        "    pred_tokens = prediction.lower().strip().split()\n",
        "    gt_tokens = ground_truth.lower().strip().split()\n",
        "\n",
        "    common = Counter(pred_tokens) & Counter(gt_tokens)\n",
        "    num_same = sum(common.values())\n",
        "\n",
        "    if num_same == 0:\n",
        "        return 0.0\n",
        "\n",
        "    precision = num_same / len(pred_tokens)\n",
        "    recall = num_same / len(gt_tokens)\n",
        "    f1 = 2 * precision * recall / (precision + recall)\n",
        "    return f1"
      ],
      "metadata": {
        "id": "uaLKlgLBIOnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import accuracy_score\n",
        "import re\n",
        "\n",
        "# --- Load Evaluation Queries ---\n",
        "def load_eval_data(path, limit=None):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    if limit:\n",
        "        return data[:limit]\n",
        "        # data = random.sample(data, min(limit, len(data)))\n",
        "\n",
        "    return data\n",
        "\n",
        "# --- Evaluate MultiHop RAG ---\n",
        "def evaluate_rag(model_fn, eval_data):\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    f1_scores = []\n",
        "\n",
        "    for example in eval_data:\n",
        "        query = example[\"query\"]\n",
        "        expected = example[\"answer\"].strip()\n",
        "        generated = model_fn(query).strip()\n",
        "\n",
        "        expected = re.sub(r'[^\\w\\s]', '', expected.lower())\n",
        "        generated = re.sub(r'[^\\w\\s]', '', generated.lower())\n",
        "\n",
        "        predictions.append(generated)\n",
        "        targets.append(expected)\n",
        "\n",
        "        f1 = f1_score(generated, expected)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "        print(f\"\\nQ: {query}\\nExpected: {expected}\\nPredicted: {generated}\\nF1: {f1:.2f}\\n{'-'*50}\")\n",
        "        avg_f1 = sum(f1_scores) / len(f1_scores)\n",
        "\n",
        "    # Simple accuracy (exact match)\n",
        "    correct = sum(p.lower() == t.lower() for p, t in zip(predictions, targets))\n",
        "    total = len(eval_data)\n",
        "    accuracy = correct / total\n",
        "    print(f\"\\n Evaluation Accuracy: {accuracy*100:.2f}% ({correct}/{total})\")\n",
        "\n",
        "    avg_f1 = sum(f1_scores) / len(f1_scores)\n",
        "    print(f\"\\n Average F1 Score: {avg_f1:.3f}\")\n",
        "\n",
        "    return accuracy, avg_f1"
      ],
      "metadata": {
        "id": "vhAi_L_Qq1CT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_data = load_eval_data(\"/content/MultiHopRAG.json\", limit=200)  # change limit as desired\n",
        "evaluate_rag(generate_answer, eval_data)"
      ],
      "metadata": {
        "id": "YELUVp2vsDXn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHyHLBY/EEO7zsLBBQz5BV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}